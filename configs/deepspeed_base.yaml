base_model_name: meta-llama/Llama-3.1-8B-Instruct
dataset_path: data/raw/wikitext_train.jsonl
output_dir: outputs/deepspeed
num_epochs: 1
batch_size: 2
learning_rate: 1.5e-5
gradient_accumulation_steps: 8
max_seq_length: 1024
lr_scheduler_type: cosine
logging_steps: 10
evaluation_steps: 50
gradient_checkpointing: true
