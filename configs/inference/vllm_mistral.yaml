model: mistralai/Mistral-7B-Instruct-v0.2
prompts: prompts/sample_prompts.jsonl
output_dir: /mnt/output/inference-benchmarks
sampling_interval: 2.0
runs:
  - name: vllm_tp1
    framework: vllm
    params:
      tensor_parallel_size: 1
      sampling:
        temperature: 0.0
        max_tokens: 256
  - name: sglang_tp2
    framework: sglang
    params:
      tensor_parallel_size: 2
      sampling:
        temperature: 0.2
        max_new_tokens: 256
  - name: lmdeploy_batch
    framework: lmdeploy
    params:
      backend:
        tp: 2
      sampling:
        temperature: 0.1
        max_new_tokens: 256
  - name: tensorrt_llm
    framework: tensorrt_llm
    params:
      engine_dir: /mnt/models/trt_engine
      runner:
        pipeline_parallel_size: 1
      sampling:
        temperature: 0.0
        max_output_len: 256
